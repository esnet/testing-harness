{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bost-dtn Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/bost-dtn\"\n",
    "# rootdir = \"/home/eadhikarla/Desktop/Brian's Project/bost-dtn\"\n",
    "\n",
    "def traverse1(path):\n",
    "    fileList = []\n",
    "    c = 0\n",
    "    for root, directories, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\"json\"):\n",
    "                fileList.append(os.path.join(root,file))\n",
    "                c+=1\n",
    "    print(f\"total files: {c}\")\n",
    "    return fileList\n",
    "\n",
    "paths = [(\"2021-06-09:14:35/pscheduler_bbr2_p1\", \"2021-06-09:14:35/pscheduler_cubic_p1\", \"2021-06-09:14:35/pscheduler_both_p16\"),\n",
    "         (\"2021-06-15:02:12/pscheduler_bbr2_p1\", \"2021-06-15:02:12/pscheduler_cubic_p1\", \"2021-06-15:02:12/pscheduler_both_p16\"),\n",
    "         (\"2021-06-16:02:12/pscheduler_bbr2_p1\", \"2021-06-16:02:12/pscheduler_cubic_p1\", \"2021-06-16:02:12/pscheduler_both_p16\")\n",
    "        ]\n",
    "\n",
    "for p in paths:\n",
    "    print(f\"--- {p} ---\")\n",
    "    pscheduler_bbr2  = os.path.join(rootdir, p[0])\n",
    "    filenames = traverse1(pscheduler_bbr2)\n",
    "\n",
    "    data_seg = []\n",
    "    tput_bbr2_p1 = []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in enumerate(data):\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = data[j]['interval']['time'] # Start time of the test\n",
    "                elif key2 in data[j].keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                    bbr2_data_seg = data[j][key2]\n",
    "                    data_seg.append( bbr2_data_seg )\n",
    "\n",
    "                    throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_bbr2_p1.append( throughput )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P1)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p1):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p1):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p1)/np.mean(tput_bbr2_p1)):.5f}  |  Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "    print(\"Data Segment (P1)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    pscheduler_cubic = os.path.join(rootdir, p[1])\n",
    "    filenames = traverse1(pscheduler_cubic)\n",
    "\n",
    "    data_seg = []\n",
    "    tput_p1_cubic = []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in enumerate(data):\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = data[j]['interval']['time'] # Start time of the test\n",
    "                elif key1 in data[j].keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                    cubic_data_seg = data[j][key1]\n",
    "                    data_seg.append( cubic_data_seg )\n",
    "\n",
    "                    throughput = (cubic_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_p1_cubic.append( throughput )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P1)\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(tput_p1_cubic):.5f}  |  Std. Dev.: {np.std(tput_p1_cubic):.5f}  |  Coef. of Variance: {(np.std(tput_p1_cubic)/np.mean(tput_p1_cubic)):.5f}  |  Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "    print(\"Data Segment (P1)\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    pscheduler_both = os.path.join(rootdir, p[2])\n",
    "    filenames = traverse1(pscheduler_both)\n",
    "\n",
    "    data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "    tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in zip(range(len(data)),data):\n",
    "                try:\n",
    "                    if \"interval\" in d.keys() and j==0:\n",
    "                        start_time = d['interval']['time'] # Start time of the test\n",
    "                    if \"streams\" in d.keys():\n",
    "                        end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                        mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                        \n",
    "                        cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                        for j in range(len(d['streams'])):\n",
    "                            if \"cubic\" in d['streams'][j]['cc']:\n",
    "                                cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                            elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                                bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        \n",
    "                        data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                        throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                        tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                        data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                        throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                        tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P16)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}  |  Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(tput_cubic_p16):.5f}  |  Std. Dev.: {np.std(tput_cubic_p16):.5f}  |  Coef. of Variance: {(np.std(tput_cubic_p16)/np.mean(tput_cubic_p16)):.5f}  |  Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "\n",
    "    print(\"Data Segment (P16)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(data_seg_sum_bbr2):.5f}  |  Std. Dev.: {np.std(data_seg_sum_bbr2):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_bbr2)/np.mean(data_seg_sum_bbr2)):.5f}  |  Variance: {np.var(data_seg_sum_bbr2):.5f}\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(data_seg_sum_cubic):.5f}  |  Std. Dev.: {np.std(data_seg_sum_cubic):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_cubic)/np.mean(data_seg_sum_cubic)):.5f}  |  Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "    print(50*\"=\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- ('2021-06-09:14:35/pscheduler_bbr2_p1', '2021-06-09:14:35/pscheduler_cubic_p1', '2021-06-09:14:35/pscheduler_both_p16') ---\n",
      "total files: 105\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 6.01623  |  Std. Dev.: 2.64542  |  Coef. of Variance: 0.43971  |  Variance: 6.99823\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 10593937.79048  |  Std. Dev.: 7304442.62257  |  Coef. of Variance: 0.68949  |  Variance: 53354882026455.64844\n",
      "\n",
      "total files: 105\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 6.52317  |  Std. Dev.: 2.94479  |  Coef. of Variance: 0.45144  |  Variance: 8.67180\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 11281322.24762  |  Std. Dev.: 7735380.38158  |  Coef. of Variance: 0.68568  |  Variance: 59836109647663.10156\n",
      "\n",
      "total files: 109\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 3.42120  |  Std. Dev.: 1.29872  |  Coef. of Variance: 0.37961  |  Variance: 1.68668\n",
      "CUBIC - Mean: 4.30749  |  Std. Dev.: 2.00634  |  Coef. of Variance: 0.46578  |  Variance: 4.02542\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 16160038.23853  |  Std. Dev.: 25749337.16830  |  Coef. of Variance: 1.59340  |  Variance: 663028364607013.12500\n",
      "CUBIC - Mean: 19844388.68807  |  Std. Dev.: 41262187.52913  |  Coef. of Variance: 2.07929  |  Variance: 1702568119688929.50000\n",
      "==================================================\n",
      "--- ('2021-06-15:02:12/pscheduler_bbr2_p1', '2021-06-15:02:12/pscheduler_cubic_p1', '2021-06-15:02:12/pscheduler_both_p16') ---\n",
      "total files: 45\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 6.73127  |  Std. Dev.: 2.84752  |  Coef. of Variance: 0.42303  |  Variance: 8.10835\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 9662307.77778  |  Std. Dev.: 6798128.56547  |  Coef. of Variance: 0.70357  |  Variance: 46214551992636.75781\n",
      "\n",
      "total files: 47\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 6.71882  |  Std. Dev.: 3.03002  |  Coef. of Variance: 0.45097  |  Variance: 9.18101\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 9722667.61702  |  Std. Dev.: 6875503.19810  |  Coef. of Variance: 0.70716  |  Variance: 47272544227031.08594\n",
      "\n",
      "total files: 50\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 5.80051  |  Std. Dev.: 2.00641  |  Coef. of Variance: 0.34590  |  Variance: 4.02567\n",
      "CUBIC - Mean: 2.36102  |  Std. Dev.: 2.01286  |  Coef. of Variance: 0.85254  |  Variance: 4.05161\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 34885604.12000  |  Std. Dev.: 61778929.03139  |  Coef. of Variance: 1.77090  |  Variance: 3816636072264945.50000\n",
      "CUBIC - Mean: 8785820.88000  |  Std. Dev.: 14797913.90272  |  Coef. of Variance: 1.68429  |  Variance: 218978255872263.18750\n",
      "==================================================\n",
      "--- ('2021-06-16:02:12/pscheduler_bbr2_p1', '2021-06-16:02:12/pscheduler_cubic_p1', '2021-06-16:02:12/pscheduler_both_p16') ---\n",
      "total files: 55\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 6.33176  |  Std. Dev.: 2.85659  |  Coef. of Variance: 0.45115  |  Variance: 8.16012\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 11637878.67273  |  Std. Dev.: 7704520.51691  |  Coef. of Variance: 0.66202  |  Variance: 59359636395545.71094\n",
      "\n",
      "total files: 54\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 6.34994  |  Std. Dev.: 3.08753  |  Coef. of Variance: 0.48623  |  Variance: 9.53285\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 11014398.94444  |  Std. Dev.: 7909237.57513  |  Coef. of Variance: 0.71808  |  Variance: 62556039019839.64844\n",
      "\n",
      "total files: 51\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 6.21103  |  Std. Dev.: 1.99755  |  Coef. of Variance: 0.32161  |  Variance: 3.99021\n",
      "CUBIC - Mean: 1.79498  |  Std. Dev.: 1.77901  |  Coef. of Variance: 0.99110  |  Variance: 3.16488\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 34174089.37255  |  Std. Dev.: 51859948.61338  |  Coef. of Variance: 1.51752  |  Variance: 2689454270182731.00000\n",
      "CUBIC - Mean: 7093619.98039  |  Std. Dev.: 13211618.30685  |  Coef. of Variance: 1.86246  |  Variance: 174546858285969.28125\n",
      "==================================================\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ESnet Hosts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/bost-dtn\"\n",
    "# rootdir = \"/home/eadhikarla/Desktop/Brian's Project/bost-dtn\"\n",
    "\n",
    "def traverse2(path):\n",
    "    fileList = []\n",
    "    c = 0\n",
    "    for root, directories, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\"json\") and file.find(\"es.net\")!=-1: # esnet host\n",
    "                fileList.append(os.path.join(root,file))\n",
    "                c+=1\n",
    "    print(f\"total files: {c}\")\n",
    "    return fileList\n",
    "\n",
    "paths = [(\"2021-06-09:14:35/pscheduler_bbr2_p1\", \"2021-06-09:14:35/pscheduler_cubic_p1\", \"2021-06-09:14:35/pscheduler_both_p16\"),\n",
    "         (\"2021-06-15:02:12/pscheduler_bbr2_p1\", \"2021-06-15:02:12/pscheduler_cubic_p1\", \"2021-06-15:02:12/pscheduler_both_p16\"),\n",
    "         (\"2021-06-16:02:12/pscheduler_bbr2_p1\", \"2021-06-16:02:12/pscheduler_cubic_p1\", \"2021-06-16:02:12/pscheduler_both_p16\")\n",
    "        ]\n",
    "\n",
    "for p in paths:\n",
    "    print(f\"--- {p} ---\")\n",
    "    pscheduler_bbr2  = os.path.join(rootdir, p[0])\n",
    "    filenames = traverse2(pscheduler_bbr2)\n",
    "\n",
    "    data_seg = []\n",
    "    tput_bbr2_p1 = []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in enumerate(data):\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = data[j]['interval']['time'] # Start time of the test\n",
    "                elif key2 in data[j].keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                    bbr2_data_seg = data[j][key2]\n",
    "                    data_seg.append( bbr2_data_seg )\n",
    "\n",
    "                    throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_bbr2_p1.append( throughput )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P1)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p1):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p1):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p1)/np.mean(tput_bbr2_p1)):.5f}  |  Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "    print(\"Data Segment (P1)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    pscheduler_cubic = os.path.join(rootdir, p[1])\n",
    "    filenames = traverse2(pscheduler_cubic)\n",
    "\n",
    "    data_seg = []\n",
    "    tput_p1_cubic = []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in enumerate(data):\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = data[j]['interval']['time'] # Start time of the test\n",
    "                elif key1 in data[j].keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                    cubic_data_seg = data[j][key1]\n",
    "                    data_seg.append( cubic_data_seg )\n",
    "\n",
    "                    throughput = (cubic_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_p1_cubic.append( throughput )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P1)\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(tput_p1_cubic):.5f}  |  Std. Dev.: {np.std(tput_p1_cubic):.5f}  |  Coef. of Variance: {(np.std(tput_p1_cubic)/np.mean(tput_p1_cubic)):.5f}  |  Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "    print(\"Data Segment (P1)\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    pscheduler_both = os.path.join(rootdir, p[2])\n",
    "    filenames = traverse2(pscheduler_both)\n",
    "\n",
    "    data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "    tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in zip(range(len(data)),data):\n",
    "                try:\n",
    "                    if \"interval\" in d.keys() and j==0:\n",
    "                        start_time = d['interval']['time'] # Start time of the test\n",
    "                    if \"streams\" in d.keys():\n",
    "                        end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                        mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                        \n",
    "                        cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                        for j in range(len(d['streams'])):\n",
    "                            if \"cubic\" in d['streams'][j]['cc']:\n",
    "                                cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                            elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                                bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        \n",
    "                        data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                        throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                        tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                        data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                        throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                        tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P16)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}  |  Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(tput_cubic_p16):.5f}  |  Std. Dev.: {np.std(tput_cubic_p16):.5f}  |  Coef. of Variance: {(np.std(tput_cubic_p16)/np.mean(tput_cubic_p16)):.5f}  |  Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "\n",
    "    print(\"Data Segment (P16)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(data_seg_sum_bbr2):.5f}  |  Std. Dev.: {np.std(data_seg_sum_bbr2):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_bbr2)/np.mean(data_seg_sum_bbr2)):.5f}  |  Variance: {np.var(data_seg_sum_bbr2):.5f}\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(data_seg_sum_cubic):.5f}  |  Std. Dev.: {np.std(data_seg_sum_cubic):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_cubic)/np.mean(data_seg_sum_cubic)):.5f}  |  Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "    print(50*\"=\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- ('2021-06-09:14:35/pscheduler_bbr2_p1', '2021-06-09:14:35/pscheduler_cubic_p1', '2021-06-09:14:35/pscheduler_both_p16') ---\n",
      "total files: 60\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 8.04496  |  Std. Dev.: 1.26181  |  Coef. of Variance: 0.15685  |  Variance: 1.59217\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 6787779.80000  |  Std. Dev.: 1042608.80432  |  Coef. of Variance: 0.15360  |  Variance: 1087033118851.19336\n",
      "\n",
      "total files: 60\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 8.78308  |  Std. Dev.: 1.42227  |  Coef. of Variance: 0.16193  |  Variance: 2.02285\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 7410541.70000  |  Std. Dev.: 1175787.44599  |  Coef. of Variance: 0.15866  |  Variance: 1382476118144.94312\n",
      "\n",
      "total files: 60\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 3.62083  |  Std. Dev.: 0.65942  |  Coef. of Variance: 0.18212  |  Variance: 0.43484\n",
      "CUBIC - Mean: 5.80462  |  Std. Dev.: 0.92845  |  Coef. of Variance: 0.15995  |  Variance: 0.86202\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 3186495.28333  |  Std. Dev.: 612129.28303  |  Coef. of Variance: 0.19210  |  Variance: 374702259148.13641\n",
      "CUBIC - Mean: 5095511.90000  |  Std. Dev.: 799137.83611  |  Coef. of Variance: 0.15683  |  Variance: 638621281104.78992\n",
      "==================================================\n",
      "--- ('2021-06-15:02:12/pscheduler_bbr2_p1', '2021-06-15:02:12/pscheduler_cubic_p1', '2021-06-15:02:12/pscheduler_both_p16') ---\n",
      "total files: 30\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 8.44357  |  Std. Dev.: 1.60358  |  Coef. of Variance: 0.18992  |  Variance: 2.57148\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 7283164.26667  |  Std. Dev.: 1135115.34968  |  Coef. of Variance: 0.15585  |  Variance: 1288486857079.39526\n",
      "\n",
      "total files: 30\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 8.74288  |  Std. Dev.: 1.40033  |  Coef. of Variance: 0.16017  |  Variance: 1.96092\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 7376319.93333  |  Std. Dev.: 1157250.23252  |  Coef. of Variance: 0.15689  |  Variance: 1339228100662.12891\n",
      "\n",
      "total files: 30\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 6.27542  |  Std. Dev.: 1.92724  |  Coef. of Variance: 0.30711  |  Variance: 3.71426\n",
      "CUBIC - Mean: 3.29367  |  Std. Dev.: 2.10902  |  Coef. of Variance: 0.64033  |  Variance: 4.44797\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 5384036.06667  |  Std. Dev.: 1696967.83277  |  Coef. of Variance: 0.31519  |  Variance: 2879699825443.19531\n",
      "CUBIC - Mean: 2799687.63333  |  Std. Dev.: 1772397.63062  |  Coef. of Variance: 0.63307  |  Variance: 3141393361025.29883\n",
      "==================================================\n",
      "--- ('2021-06-16:02:12/pscheduler_bbr2_p1', '2021-06-16:02:12/pscheduler_cubic_p1', '2021-06-16:02:12/pscheduler_both_p16') ---\n",
      "total files: 30\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 8.65769  |  Std. Dev.: 1.38476  |  Coef. of Variance: 0.15995  |  Variance: 1.91755\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 7304069.13333  |  Std. Dev.: 1144987.26387  |  Coef. of Variance: 0.15676  |  Variance: 1310995834426.71533\n",
      "\n",
      "total files: 30\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 8.78091  |  Std. Dev.: 1.42333  |  Coef. of Variance: 0.16209  |  Variance: 2.02588\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 7408789.36667  |  Std. Dev.: 1177198.07625  |  Coef. of Variance: 0.15889  |  Variance: 1385795310717.03223\n",
      "\n",
      "total files: 26\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 6.38694  |  Std. Dev.: 2.17541  |  Coef. of Variance: 0.34060  |  Variance: 4.73240\n",
      "CUBIC - Mean: 2.78364  |  Std. Dev.: 2.00765  |  Coef. of Variance: 0.72123  |  Variance: 4.03067\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 5487991.73077  |  Std. Dev.: 1899940.52405  |  Coef. of Variance: 0.34620  |  Variance: 3609773994911.19629\n",
      "CUBIC - Mean: 2370550.30769  |  Std. Dev.: 1687408.41260  |  Coef. of Variance: 0.71182  |  Variance: 2847347150903.98242\n",
      "==================================================\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Non-ESnet Hosts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/bost-dtn\"\n",
    "# rootdir = \"/home/eadhikarla/Desktop/Brian's Project/bost-dtn\"\n",
    "\n",
    "def traverse3(path):\n",
    "    fileList = []\n",
    "    c = 0\n",
    "    for root, directories, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\"json\") and file.find(\"es.net\")==-1: # non-esnet host\n",
    "                fileList.append(os.path.join(root,file))\n",
    "                c+=1\n",
    "    print(f\"total files: {c}\")\n",
    "    return fileList\n",
    "\n",
    "paths = [(\"2021-06-09:14:35/pscheduler_bbr2_p1\", \"2021-06-09:14:35/pscheduler_cubic_p1\", \"2021-06-09:14:35/pscheduler_both_p16\"),\n",
    "         (\"2021-06-15:02:12/pscheduler_bbr2_p1\", \"2021-06-15:02:12/pscheduler_cubic_p1\", \"2021-06-15:02:12/pscheduler_both_p16\"),\n",
    "         (\"2021-06-16:02:12/pscheduler_bbr2_p1\", \"2021-06-16:02:12/pscheduler_cubic_p1\", \"2021-06-16:02:12/pscheduler_both_p16\")\n",
    "        ]\n",
    "\n",
    "for p in paths:\n",
    "    print(f\"--- {p} ---\")\n",
    "    pscheduler_bbr2  = os.path.join(rootdir, p[0])\n",
    "    filenames = traverse3(pscheduler_bbr2)\n",
    "\n",
    "    data_seg = []\n",
    "    tput_bbr2_p1 = []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in enumerate(data):\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = data[j]['interval']['time'] # Start time of the test\n",
    "                elif key2 in data[j].keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                    bbr2_data_seg = data[j][key2]\n",
    "                    data_seg.append( bbr2_data_seg )\n",
    "\n",
    "                    throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_bbr2_p1.append( throughput )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P1)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p1):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p1):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p1)/np.mean(tput_bbr2_p1)):.5f}  |  Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "    print(\"Data Segment (P1)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    pscheduler_cubic = os.path.join(rootdir, p[1])\n",
    "    filenames = traverse3(pscheduler_cubic)\n",
    "\n",
    "    data_seg = []\n",
    "    tput_p1_cubic = []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in enumerate(data):\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = data[j]['interval']['time'] # Start time of the test\n",
    "                elif key1 in data[j].keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                    cubic_data_seg = data[j][key1]\n",
    "                    data_seg.append( cubic_data_seg )\n",
    "\n",
    "                    throughput = (cubic_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_p1_cubic.append( throughput )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P1)\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(tput_p1_cubic):.5f}  |  Std. Dev.: {np.std(tput_p1_cubic):.5f}  |  Coef. of Variance: {(np.std(tput_p1_cubic)/np.mean(tput_p1_cubic)):.5f}  |  Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "    print(\"Data Segment (P1)\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    pscheduler_both = os.path.join(rootdir, p[2])\n",
    "    filenames = traverse3(pscheduler_both)\n",
    "\n",
    "    data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "    tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in zip(range(len(data)),data):\n",
    "                try:\n",
    "                    if \"interval\" in d.keys() and j==0:\n",
    "                        start_time = d['interval']['time'] # Start time of the test\n",
    "                    if \"streams\" in d.keys():\n",
    "                        end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                        mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                        \n",
    "                        cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                        for j in range(len(d['streams'])):\n",
    "                            if \"cubic\" in d['streams'][j]['cc']:\n",
    "                                cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                            elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                                bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        \n",
    "                        data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                        throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                        tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                        data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                        throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                        tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P16)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}  |  Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(tput_cubic_p16):.5f}  |  Std. Dev.: {np.std(tput_cubic_p16):.5f}  |  Coef. of Variance: {(np.std(tput_cubic_p16)/np.mean(tput_cubic_p16)):.5f}  |  Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "\n",
    "    print(\"Data Segment (P16)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(data_seg_sum_bbr2):.5f}  |  Std. Dev.: {np.std(data_seg_sum_bbr2):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_bbr2)/np.mean(data_seg_sum_bbr2)):.5f}  |  Variance: {np.var(data_seg_sum_bbr2):.5f}\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(data_seg_sum_cubic):.5f}  |  Std. Dev.: {np.std(data_seg_sum_cubic):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_cubic)/np.mean(data_seg_sum_cubic)):.5f}  |  Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "    print(50*\"=\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- ('2021-06-09:14:35/pscheduler_bbr2_p1', '2021-06-09:14:35/pscheduler_cubic_p1', '2021-06-09:14:35/pscheduler_both_p16') ---\n",
      "total files: 45\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 3.31125  |  Std. Dev.: 1.18396  |  Coef. of Variance: 0.35756  |  Variance: 1.40175\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 15668815.11111  |  Std. Dev.: 8830355.74326  |  Coef. of Variance: 0.56356  |  Variance: 77975182552479.25000\n",
      "\n",
      "total files: 45\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 3.50997  |  Std. Dev.: 1.28378  |  Coef. of Variance: 0.36575  |  Variance: 1.64809\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 16442362.97778  |  Std. Dev.: 9547810.75456  |  Coef. of Variance: 0.58068  |  Variance: 91160690204895.26562\n",
      "\n",
      "total files: 49\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 3.17675  |  Std. Dev.: 1.76380  |  Coef. of Variance: 0.55522  |  Variance: 3.11100\n",
      "CUBIC - Mean: 2.47428  |  Std. Dev.: 1.33931  |  Coef. of Variance: 0.54129  |  Variance: 1.79375\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 32046009.20408  |  Std. Dev.: 31874435.08762  |  Coef. of Variance: 0.99465  |  Variance: 1015979612155029.50000\n",
      "CUBIC - Mean: 37904237.81633  |  Std. Dev.: 56515863.11579  |  Coef. of Variance: 1.49102  |  Variance: 3194042783722711.00000\n",
      "==================================================\n",
      "--- ('2021-06-15:02:12/pscheduler_bbr2_p1', '2021-06-15:02:12/pscheduler_cubic_p1', '2021-06-15:02:12/pscheduler_both_p16') ---\n",
      "total files: 15\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 3.30666  |  Std. Dev.: 1.26102  |  Coef. of Variance: 0.38136  |  Variance: 1.59016\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 14420594.80000  |  Std. Dev.: 10104688.97022  |  Coef. of Variance: 0.70071  |  Variance: 102104739184978.82812\n",
      "\n",
      "total files: 17\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 3.14694  |  Std. Dev.: 1.39082  |  Coef. of Variance: 0.44196  |  Variance: 1.93437\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 13863281.17647  |  Std. Dev.: 10073298.25371  |  Coef. of Variance: 0.72662  |  Variance: 101471337708255.32812\n",
      "\n",
      "total files: 20\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 5.08815  |  Std. Dev.: 1.90971  |  Coef. of Variance: 0.37533  |  Variance: 3.64701\n",
      "CUBIC - Mean: 0.96206  |  Std. Dev.: 0.44187  |  Coef. of Variance: 0.45930  |  Variance: 0.19525\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 79137956.20000  |  Std. Dev.: 79205341.92783  |  Coef. of Variance: 1.00085  |  Variance: 6273486189903734.00000\n",
      "CUBIC - Mean: 17765020.75000  |  Std. Dev.: 20207840.86595  |  Coef. of Variance: 1.13751  |  Variance: 408356832463440.12500\n",
      "==================================================\n",
      "--- ('2021-06-16:02:12/pscheduler_bbr2_p1', '2021-06-16:02:12/pscheduler_cubic_p1', '2021-06-16:02:12/pscheduler_both_p16') ---\n",
      "total files: 25\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 3.54063  |  Std. Dev.: 1.16998  |  Coef. of Variance: 0.33044  |  Variance: 1.36886\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 16838450.12000  |  Std. Dev.: 8912562.79317  |  Coef. of Variance: 0.52930  |  Variance: 79433775542209.87500\n",
      "\n",
      "total files: 24\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 3.31122  |  Std. Dev.: 1.51517  |  Coef. of Variance: 0.45759  |  Variance: 2.29574\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 15521410.91667  |  Std. Dev.: 10122013.69315  |  Coef. of Variance: 0.65213  |  Variance: 102455161204284.92188\n",
      "\n",
      "total files: 25\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 6.02808  |  Std. Dev.: 1.77558  |  Coef. of Variance: 0.29455  |  Variance: 3.15267\n",
      "CUBIC - Mean: 0.76678  |  Std. Dev.: 0.43670  |  Coef. of Variance: 0.56952  |  Variance: 0.19070\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 64007630.92000  |  Std. Dev.: 61130061.82824  |  Coef. of Variance: 0.95504  |  Variance: 3736884459124306.50000\n",
      "CUBIC - Mean: 12005612.44000  |  Std. Dev.: 17486766.34843  |  Coef. of Variance: 1.45655  |  Variance: 305786997324688.75000\n",
      "==================================================\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/\"\n",
    "\n",
    "def _filereader(path, verbose=False):\n",
    "    with open(path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        tempList = []\n",
    "        # This skips the first row of the CSV file.\n",
    "        next(csv_reader)\n",
    "\n",
    "        for enum, row in enumerate(csv_reader):\n",
    "            if verbose:\n",
    "                print(f'{enum}.\\t{row[0]}, {row[1]}, {row[2]}')\n",
    "            tempList.append( (row[0],row[1],row[2]) )\n",
    "            line_count += 1\n",
    "        print(f'Processed {line_count} lines.')\n",
    "    return tempList\n",
    "\n",
    "rtt_data = _filereader(os.path.join(rootdir,'test-hosts.bost-dtn.csv'),False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.\tbost-pt1.es.net, 1.0ms, 10Gbps\n",
      "1.\tnewy-pt1.es.net, 5.0ms, 10Gbps\n",
      "2.\teqx-ash-pt1.es.net, 10.0ms, 10Gbps\n",
      "3.\tstar-pt1.es.net, 20.0ms, 10Gbps\n",
      "4.\tkans-pt1.es.net, 31.0ms, 10Gbps\n",
      "5.\tdenv-pt1.es.net, 41.0ms, 10Gbps\n",
      "6.\talbq-pt1.es.net, 49.0ms, 10Gbps\n",
      "7.\tsnla-pt1.es.net, 50.0ms, 1Gbps\n",
      "8.\tsacr-pt1.es.net, 61.0ms, 10Gbps\n",
      "9.\tcern-773-pt1.es.net, 87.0ms, 10Gbps\n",
      "10.\taofa-pt1.es.net, 5.0ms, 10Gbps\n",
      "11.\tpppl-pt1.es.net, 6.0ms, 10Gbps\n",
      "12.\twash-pt1.es.net, 9.0ms, 10Gbps\n",
      "13.\tbois-pt1.es.net, 57.0ms, 10Gbps\n",
      "14.\tpsl01.pic.es, 119.0ms, 1Gbps\n",
      "15.\tpsb.hpc.utfsm.cl, 170.0ms, 1Gbps\n",
      "16.\tnsw-brwy-ps1.aarnet.net.au, 230.0ms,  10Gbps\n",
      "17.\tperfsonar-bw.sprace.org.br, 140.0ms,  10Gbps\n",
      "18.\tperfsonar-latency.grid.surfsara.nl, 73.0ms, 10Gbps\n",
      "19.\tbtw-bw.t1.grid.kiae.ru, 109.0ms, 1Gbps\n",
      "20.\tfiona.sce.pennren.net, 13.0ms, 10Gbps\n",
      "21.\tperfsonar.oshean.org, 9.0ms, 10Gbps\n",
      "22.\ttau.ijs.si, 102.0ms, 10Gbps\n",
      "23.\tlcgps02.gridpp.rl.ac.uk, 75.0ms, 10Gbps\n",
      "24.\tperfsonar.na.infn.it, 108.0ms,  10Gbps\n",
      "25.\tperfsonar2.icepp.jp, 173.0ms, 10Gbps\n",
      "26.\tpygrid-sonar2.lancs.ac.uk, 93.0ms, 10Gbps\n",
      "Processed 27 lines.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def traverse_less_than_30(path):\n",
    "    fileList = []\n",
    "    c = 0\n",
    "    for root, directories, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.startswith(\"ss\") and file.endswith(\"json\"):\n",
    "                for enum, rtt_file in enumerate(rtt_data):\n",
    "                    if file.find(rtt_file[0])!=-1 and float(rtt_file[1].split(\"ms\")[0])<30:\n",
    "                        print(file, enum)\n",
    "                        fileList.append(os.path.join(root,file))\n",
    "                        c+=1\n",
    "    print(f\"Total files: {c}\")\n",
    "    return fileList\n",
    "\n",
    "x = traverse_less_than_30(os.path.join(rootdir,\"bost-dtn-10G/2021-07-30:19:21/pscheduler_both_p16\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ss:eqx-ash-pt1.es.net:4.json 2\n",
      "ss:newy-pt1.es.net:2.json 1\n",
      "ss:pppl-pt1.es.net:3.json 11\n",
      "ss:aofa-pt1.es.net:4.json 10\n",
      "ss:aofa-pt1.es.net:5.json 10\n",
      "ss:bost-pt1.es.net:1.json 0\n",
      "ss:pppl-pt1.es.net:2.json 11\n",
      "ss:newy-pt1.es.net:3.json 1\n",
      "ss:eqx-ash-pt1.es.net:5.json 2\n",
      "ss:newy-pt1.es.net:4.json 1\n",
      "ss:pppl-pt1.es.net:5.json 11\n",
      "ss:aofa-pt1.es.net:2.json 10\n",
      "ss:eqx-ash-pt1.es.net:2.json 2\n",
      "ss:eqx-ash-pt1.es.net:3.json 2\n",
      "ss:aofa-pt1.es.net:3.json 10\n",
      "ss:star-pt1.es.net:1.json 3\n",
      "ss:wash-pt1.es.net:1.json 12\n",
      "ss:pppl-pt1.es.net:4.json 11\n",
      "ss:newy-pt1.es.net:5.json 1\n",
      "ss:star-pt1.es.net:2.json 3\n",
      "ss:bost-pt1.es.net:4.json 0\n",
      "ss:wash-pt1.es.net:2.json 12\n",
      "ss:eqx-ash-pt1.es.net:1.json 2\n",
      "ss:wash-pt1.es.net:3.json 12\n",
      "ss:bost-pt1.es.net:5.json 0\n",
      "ss:star-pt1.es.net:3.json 3\n",
      "ss:aofa-pt1.es.net:1.json 10\n",
      "ss:star-pt1.es.net:4.json 3\n",
      "ss:bost-pt1.es.net:2.json 0\n",
      "ss:wash-pt1.es.net:4.json 12\n",
      "ss:pppl-pt1.es.net:1.json 11\n",
      "ss:newy-pt1.es.net:1.json 1\n",
      "ss:wash-pt1.es.net:5.json 12\n",
      "ss:bost-pt1.es.net:3.json 0\n",
      "ss:star-pt1.es.net:5.json 3\n",
      "Total files: 35\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6e2000e74d83ab38a7a73c8353776f8595191be383670aed524d2a15b88663d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}