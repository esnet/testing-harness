{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\"\"\"\n",
    "ESNET TESTBED EXPERIMENTS\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/experiment2\"\n",
    "rootdir = \"/Users/eashan22/Desktop/experiment2\"\n",
    "\n",
    "def traverse_less_than_30(path):\n",
    "    fileList = []\n",
    "    c = 0\n",
    "    for root, directories, files in os.walk(path):\n",
    "        for file in files:\n",
    "            # if file.endswith(\"json\"):\n",
    "            if file.endswith(\"json\") and float( file.split(\":\")[3].split(\".json\")[0].split(\"ms\")[0] )<30: # for filess where RTT is less than 30 \n",
    "                fileList.append(os.path.join(root,file))\n",
    "                c+=1\n",
    "    print(f\"Total files: {c}\")\n",
    "    return fileList\n",
    "\n",
    "def traverse_greater_than_30(path):\n",
    "    fileList = []\n",
    "    c = 0\n",
    "    for root, directories, files in os.walk(path):\n",
    "        for file in files:\n",
    "            # if file.endswith(\"json\"):\n",
    "            if file.endswith(\"json\") and float( file.split(\":\")[3].split(\".json\")[0].split(\"ms\")[0] )>30: # for filess where RTT is greater than 30 \n",
    "                fileList.append(os.path.join(root,file))\n",
    "                c+=1\n",
    "    print(f\"Total files: {c}\")\n",
    "    return fileList\n",
    "\n",
    "paths = [\n",
    "        (#\"0.1-loss.v1/pscheduler_bbr2_p1\",\n",
    "        # \"0.001-loss/pscheduler_bbr2_p16\",\n",
    "        # \"0.1-loss.v1/pscheduler_bbr2_p16\",\n",
    "        \"0.01-loss.v1/pscheduler_bbr2_p1\",\n",
    "        \"0.01-loss.v1/pscheduler_bbr2_p16\",\n",
    "        \"no-loss/pscheduler_bbr2_p1\",\n",
    "        \"no-loss/pscheduler_bbr2_p16\",\n",
    "        \"no-loss/pscheduler_bbr2_p16_v2\",),\n",
    "\n",
    "        (#\"0.1-loss/pscheduler_cubic_p16\",\n",
    "        # \"0.1-loss.v1/pscheduler_cubic_p1\",   \n",
    "        # \"0.1-loss.v1/pscheduler_cubic_p16\",\n",
    "        # \"0.001-loss/pscheduler_cubic_p16\",\n",
    "        \"0.01-loss.v1/pscheduler_cubic_p1\",  \n",
    "        \"0.01-loss.v1/pscheduler_cubic_p16\",\n",
    "        \"no-loss/pscheduler_cubic_p1\",\n",
    "        \"no-loss/pscheduler_cubic_p16\",\n",
    "        \"no-loss/pscheduler_cubic_p16_v2\"),\n",
    "\n",
    "        (#\"0.001-loss/pscheduler_both_p16\",\n",
    "        # \"0.1-loss.v1/pscheduler_both_p1\",\n",
    "        # \"0.1-loss.v1/pscheduler_both_p16\",\n",
    "        # \"0.1-loss.v2/pscheduler_both_p16\", \n",
    "        # \"0.1-loss.v2/pscheduler_both_p16_bbr1\",\n",
    "        \"0.01-loss.v1/pscheduler_both_p1\",\n",
    "        \"0.01-loss.v1/pscheduler_both_p16\",\n",
    "        \"no-loss/pscheduler_both_p1\",\n",
    "        \"no-loss/pscheduler_both_p16\",\n",
    "        \"no-loss/pscheduler_both_p16_v2\")\n",
    "        ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# =================================================================================\n",
    "# TRAVERSE LESS THAN 30 RTT\n",
    "# =================================================================================\n",
    "\n",
    "print(\"\\n\")\n",
    "for q1 in paths[0]:\n",
    "    print(f\"============================ {q1} ============================\")\n",
    "    pscheduler_bbr2  = os.path.join(rootdir, q1)\n",
    "    filenames = traverse_less_than_30(pscheduler_bbr2)\n",
    "\n",
    "    data_seg = []\n",
    "    tput_bbr2_p1 = []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in enumerate(data):\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = data[j]['interval']['time'] # Start time of the test\n",
    "                elif key2 in data[j].keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                    bbr2_data_seg = data[j][key2]\n",
    "                    data_seg.append( bbr2_data_seg )\n",
    "\n",
    "                    throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_bbr2_p1.append( throughput )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P1)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p1):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p1)/np.mean(tput_bbr2_p1)):.5f}\")\n",
    "    # print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p1):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p1):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p1)/np.mean(tput_bbr2_p1)):.5f}  |  Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "    # print(\"Data Segment (P1)\")\n",
    "    # print(f\"BBRv2 - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "    print(\"\")\n",
    "\n",
    "for q2 in paths[1]:\n",
    "    print(f\"============================ {q2} ============================\")\n",
    "    pscheduler_cubic = os.path.join(rootdir, q2)\n",
    "    filenames = traverse_less_than_30(pscheduler_cubic)\n",
    "\n",
    "    data_seg = []\n",
    "    tput_p1_cubic = []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in enumerate(data):\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = data[j]['interval']['time'] # Start time of the test\n",
    "                elif key1 in data[j].keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                    cubic_data_seg = data[j][key1]\n",
    "                    data_seg.append( cubic_data_seg )\n",
    "\n",
    "                    throughput = (cubic_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_p1_cubic.append( throughput )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P1)\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(tput_p1_cubic):.5f}  |  Coef. of Variance: {(np.std(tput_p1_cubic)/np.mean(tput_p1_cubic)):.5f}\")\n",
    "    # print(f\"CUBIC - Mean: {np.mean(tput_p1_cubic):.5f}  |  Std. Dev.: {np.std(tput_p1_cubic):.5f}  |  Coef. of Variance: {(np.std(tput_p1_cubic)/np.mean(tput_p1_cubic)):.5f}  |  Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "    # print(\"Data Segment (P1)\")\n",
    "    # print(f\"CUBIC - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "    print(\"\")\n",
    "\n",
    "for q3 in paths[2]:\n",
    "    print(f\"============================ {q3} ============================\")\n",
    "    pscheduler_both = os.path.join(rootdir, q3)\n",
    "    filenames = traverse_less_than_30(pscheduler_both)\n",
    "\n",
    "    data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "    tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in zip(range(len(data)),data):\n",
    "                try:\n",
    "                    if \"interval\" in d.keys() and j==0:\n",
    "                        start_time = d['interval']['time'] # Start time of the test\n",
    "                    if \"streams\" in d.keys():\n",
    "                        end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                        mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                        \n",
    "                        cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                        for j in range(len(d['streams'])):\n",
    "                            if \"cubic\" in d['streams'][j]['cc']:\n",
    "                                cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                            elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                                bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        \n",
    "                        data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                        throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                        tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                        data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                        throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                        tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P16)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(tput_cubic_p16):.5f}  |  Coef. of Variance: {(np.std(tput_cubic_p16)/np.mean(tput_cubic_p16)):.5f}\")\n",
    "    # print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}  |  Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "    # print(f\"CUBIC - Mean: {np.mean(tput_cubic_p16):.5f}  |  Std. Dev.: {np.std(tput_cubic_p16):.5f}  |  Coef. of Variance: {(np.std(tput_cubic_p16)/np.mean(tput_cubic_p16)):.5f}  |  Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "\n",
    "    # print(\"Data Segment (P16)\")\n",
    "    # print(f\"BBRv2 - Mean: {np.mean(data_seg_sum_bbr2):.5f}  |  Std. Dev.: {np.std(data_seg_sum_bbr2):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_bbr2)/np.mean(data_seg_sum_bbr2)):.5f}  |  Variance: {np.var(data_seg_sum_bbr2):.5f}\")\n",
    "    # print(f\"CUBIC - Mean: {np.mean(data_seg_sum_cubic):.5f}  |  Std. Dev.: {np.std(data_seg_sum_cubic):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_cubic)/np.mean(data_seg_sum_cubic)):.5f}  |  Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "    print(50*\"=\")\n",
    "\n",
    "\n",
    "print(50*\"*\")\n",
    "print(50*\"*\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "============================ 0.01-loss.v1/pscheduler_bbr2_p1 ============================\n",
      "-- ss:10.201.1.2:4:20.0ms.json\n",
      "20.0\n",
      "-- ss:10.201.1.2:4:4.0ms.json\n",
      "4.0\n",
      "-- ss:10.201.1.2:3:10.0ms.json\n",
      "10.0\n",
      "-- ss:10.201.1.2:1:20.0ms.json\n",
      "20.0\n",
      "-- ss:10.201.1.2:1:4.0ms.json\n",
      "4.0\n",
      "-- ss:10.201.1.2:2:50.0ms.json\n",
      "50.0\n",
      "-- ss:10.201.1.2:1:50.0ms.json\n",
      "50.0\n",
      "-- ss:10.201.1.2:2:20.0ms.json\n",
      "20.0\n",
      "-- ss:10.201.1.2:3:4.0ms.json\n",
      "4.0\n",
      "-- ss:10.201.1.2:5:10.0ms.json\n",
      "10.0\n",
      "-- ss:10.201.1.2:4:50.0ms.json\n",
      "50.0\n",
      "-- ss:10.201.1.2:3:50.0ms.json\n",
      "50.0\n",
      "-- ss:10.201.1.2:2:10.0ms.json\n",
      "10.0\n",
      "-- ss:10.201.1.2:5:20.0ms.json\n",
      "20.0\n",
      "-- ss:10.201.1.2:5:4.0ms.json\n",
      "4.0\n",
      "-- ss:10.201.1.2:2:4.0ms.json\n",
      "4.0\n",
      "-- ss:10.201.1.2:5:50.0ms.json\n",
      "50.0\n",
      "-- ss:10.201.1.2:4:10.0ms.json\n",
      "10.0\n",
      "-- ss:10.201.1.2:1:10.0ms.json\n",
      "10.0\n",
      "-- ss:10.201.1.2:3:20.0ms.json\n",
      "20.0\n",
      "Total files: 15\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 2.34768  |  Coef. of Variance: 0.00171\n",
      "\n",
      "============================ 0.01-loss.v1/pscheduler_bbr2_p16 ============================\n",
      "-- ss:10.201.1.2:4:20.0ms.json\n",
      "20.0\n",
      "-- ss:10.201.1.2:4:4.0ms.json\n",
      "4.0\n",
      "-- ss:10.201.1.2:3:10.0ms.json\n",
      "10.0\n",
      "-- ss:10.201.1.2:1:20.0ms.json\n",
      "20.0\n",
      "-- ss:10.201.1.2:1:4.0ms.json\n",
      "4.0\n",
      "-- ss:10.201.1.2:2:50.0ms.json\n",
      "50.0\n",
      "-- ss:10.201.1.2:1:50.0ms.json\n",
      "50.0\n",
      "-- ss:10.201.1.2:2:20.0ms.json\n",
      "20.0\n",
      "-- ss:10.201.1.2:3:4.0ms.json\n",
      "4.0\n",
      "-- ss:10.201.1.2:5:10.0ms.json\n",
      "10.0\n",
      "-- ss:10.201.1.2:4:50.0ms.json\n",
      "50.0\n",
      "-- ss:10.201.1.2:3:50.0ms.json\n",
      "50.0\n",
      "-- ss:10.201.1.2:2:10.0ms.json\n",
      "10.0\n",
      "-- ss:10.201.1.2:5:20.0ms.json\n",
      "20.0\n",
      "-- ss:10.201.1.2:5:4.0ms.json\n",
      "4.0\n",
      "-- ss:10.201.1.2:2:4.0ms.json\n",
      "4.0\n",
      "-- ss:10.201.1.2:5:50.0ms.json\n",
      "50.0\n",
      "-- ss:10.201.1.2:4:10.0ms.json\n",
      "10.0\n",
      "-- ss:10.201.1.2:1:10.0ms.json\n",
      "10.0\n",
      "-- ss:10.201.1.2:3:20.0ms.json\n",
      "20.0\n",
      "Total files: 15\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 9.75861  |  Coef. of Variance: 0.00525\n",
      "\n",
      "============================ no-loss/pscheduler_bbr2_p1 ============================\n",
      "-- pre-netem:10.201.1.2:4:50.0ms\n",
      "50.0\n",
      "-- pre-netem:10.201.1.2:5:10.0ms\n",
      "10.0\n",
      "-- pre-src-cmd:10.201.1.2:1:20.0ms\n",
      "20.0\n",
      "-- ss:10.201.1.2:3:50.0ms\n",
      "50.0\n",
      "-- pre-src-cmd:10.201.1.2:5:50.0ms\n",
      "50.0\n",
      "-- src-cmd:10.201.1.2:3:100.0ms\n",
      "100.0\n",
      "-- pre-src-cmd:10.201.1.2:4:10.0ms\n",
      "10.0\n",
      "-- ss:10.201.1.2:2:10.0ms\n",
      "10.0\n",
      "-- src-cmd:10.201.1.2:5:10.0ms\n",
      "10.0\n",
      "-- ss:10.201.1.2:4:20.0ms.json\n",
      "20.0\n",
      "-- ss:10.201.1.2:4:4.0ms.json\n",
      "4.0\n",
      "-- src-cmd:10.201.1.2:4:50.0ms\n",
      "50.0\n",
      "-- ss:10.201.1.2:3:10.0ms.json\n",
      "10.0\n",
      "-- ss:10.201.1.2:1:50.0ms\n",
      "50.0\n",
      "-- src-cmd:10.201.1.2:2:100.0ms\n",
      "100.0\n",
      "-- pre-src-cmd:10.201.1.2:3:20.0ms\n",
      "20.0\n",
      "-- ss:10.201.1.2:5:20.0ms\n",
      "20.0\n",
      "-- src-cmd:10.201.1.2:2:20.0ms\n",
      "20.0\n",
      "-- ss:10.201.1.2:1:100.0ms\n",
      "100.0\n",
      "-- ss:10.201.1.2:1:20.0ms.json\n",
      "20.0\n",
      "-- ss:10.201.1.2:1:4.0ms.json\n",
      "4.0\n",
      "-- ss:10.201.1.2:2:50.0ms.json\n",
      "50.0\n",
      "-- pre-netem:10.201.1.2:2:20.0ms\n",
      "20.0\n",
      "-- pre-netem:10.201.1.2:5:100.0ms\n",
      "100.0\n",
      "-- ss:10.201.1.2:1:50.0ms.json\n",
      "50.0\n",
      "-- src-cmd:10.201.1.2:3:10.0ms\n",
      "10.0\n",
      "-- jobmeta.json\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d2c47dfec109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"============================ {q1} ============================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpscheduler_bbr2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraverse_less_than_30\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpscheduler_bbr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-c16ae82f4019>\u001b[0m in \u001b[0;36mtraverse_less_than_30\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# if file.endswith(\"json\"):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for filess where RTT is less than 30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mfileList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# =================================================================================\n",
    "# TRAVERSE GREATER THAN 30 RTT\n",
    "# =================================================================================\n",
    "\n",
    "print(\"\\n\")\n",
    "for q1 in paths[0]:\n",
    "    print(f\"============================ {q1} ============================\")\n",
    "    pscheduler_bbr2  = os.path.join(rootdir, q1)\n",
    "    filenames = traverse_greater_than_30(pscheduler_bbr2)\n",
    "\n",
    "    data_seg = []\n",
    "    tput_bbr2_p1 = []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in enumerate(data):\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = data[j]['interval']['time'] # Start time of the test\n",
    "                elif key2 in data[j].keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                    bbr2_data_seg = data[j][key2]\n",
    "                    data_seg.append( bbr2_data_seg )\n",
    "\n",
    "                    throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_bbr2_p1.append( throughput )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P1)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p1):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p1)/np.mean(tput_bbr2_p1)):.5f}\")\n",
    "    # print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p1):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p1):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p1)/np.mean(tput_bbr2_p1)):.5f}  |  Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "    # print(\"Data Segment (P1)\")\n",
    "    # print(f\"BBRv2 - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "    print(\"\")\n",
    "\n",
    "for q2 in paths[1]:\n",
    "    print(f\"============================ {q2} ============================\")\n",
    "    pscheduler_cubic = os.path.join(rootdir, q2)\n",
    "    filenames = traverse_greater_than_30(pscheduler_cubic)\n",
    "\n",
    "    data_seg = []\n",
    "    tput_p1_cubic = []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in enumerate(data):\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = data[j]['interval']['time'] # Start time of the test\n",
    "                elif key1 in data[j].keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                    cubic_data_seg = data[j][key1]\n",
    "                    data_seg.append( cubic_data_seg )\n",
    "\n",
    "                    throughput = (cubic_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_p1_cubic.append( throughput )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P1)\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(tput_p1_cubic):.5f}  |  Coef. of Variance: {(np.std(tput_p1_cubic)/np.mean(tput_p1_cubic)):.5f}\")\n",
    "    # print(f\"CUBIC - Mean: {np.mean(tput_p1_cubic):.5f}  |  Std. Dev.: {np.std(tput_p1_cubic):.5f}  |  Coef. of Variance: {(np.std(tput_p1_cubic)/np.mean(tput_p1_cubic)):.5f}  |  Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "    # print(\"Data Segment (P1)\")\n",
    "    # print(f\"CUBIC - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "    print(\"\")\n",
    "\n",
    "for q3 in paths[2]:\n",
    "    print(f\"============================ {q3} ============================\")\n",
    "    pscheduler_both = os.path.join(rootdir, q3)\n",
    "    filenames = traverse_greater_than_30(pscheduler_both)\n",
    "\n",
    "    data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "    tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "    key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "    for i,f in enumerate(filenames):\n",
    "        try:\n",
    "            path = Path(f)\n",
    "            data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "            throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            for j,d in zip(range(len(data)),data):\n",
    "                try:\n",
    "                    if \"interval\" in d.keys() and j==0:\n",
    "                        start_time = d['interval']['time'] # Start time of the test\n",
    "                    if \"streams\" in d.keys():\n",
    "                        end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                        mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                        \n",
    "                        cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                        for j in range(len(d['streams'])):\n",
    "                            if \"cubic\" in d['streams'][j]['cc']:\n",
    "                                cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                            elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                                bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        \n",
    "                        data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                        throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                        tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                        data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                        throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                        tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(\"Throughput (P16)\")\n",
    "    print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}\")\n",
    "    print(f\"CUBIC - Mean: {np.mean(tput_cubic_p16):.5f}  |  Coef. of Variance: {(np.std(tput_cubic_p16)/np.mean(tput_cubic_p16)):.5f}\")\n",
    "    # print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}  |  Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "    # print(f\"CUBIC - Mean: {np.mean(tput_cubic_p16):.5f}  |  Std. Dev.: {np.std(tput_cubic_p16):.5f}  |  Coef. of Variance: {(np.std(tput_cubic_p16)/np.mean(tput_cubic_p16)):.5f}  |  Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "\n",
    "    # print(\"Data Segment (P16)\")\n",
    "    # print(f\"BBRv2 - Mean: {np.mean(data_seg_sum_bbr2):.5f}  |  Std. Dev.: {np.std(data_seg_sum_bbr2):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_bbr2)/np.mean(data_seg_sum_bbr2)):.5f}  |  Variance: {np.var(data_seg_sum_bbr2):.5f}\")\n",
    "    # print(f\"CUBIC - Mean: {np.mean(data_seg_sum_cubic):.5f}  |  Std. Dev.: {np.std(data_seg_sum_cubic):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_cubic)/np.mean(data_seg_sum_cubic)):.5f}  |  Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "    print(50*\"=\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# import os\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "\n",
    "# rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/experiment2\"\n",
    "# # rootdir = \"/home/eadhikarla/Desktop/Brian's Project/bost-dtn\"\n",
    "\n",
    "# def traverse(path):\n",
    "#     fileList = []\n",
    "#     c = 0\n",
    "#     for root, directories, files in os.walk(path):\n",
    "#         for file in files:\n",
    "#             if file.endswith(\"json\") and float(file.split(\":\")[3].split(\".json\")[0].split(\"ms\")[0])<30: # for filess where RTT is less than 30 \n",
    "#                 fileList.append(os.path.join(root,file))\n",
    "#                 c+=1\n",
    "#     print(f\"Total files: {c}\")\n",
    "#     # return fileList\n",
    "\n",
    "# p = os.path.join(rootdir, \"no-loss/pscheduler_bbr2_p1\")\n",
    "# traverse(p)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# paths = [\n",
    "#         (\"0.1-loss.v1/pscheduler_bbr2_p1\",\n",
    "#         \"0.001-loss/pscheduler_bbr2_p16\",\n",
    "#         \"0.1-loss.v1/pscheduler_bbr2_p16\",\n",
    "#         \"0.01-loss.v1/pscheduler_bbr2_p1\",\n",
    "#         \"0.01-loss.v1/pscheduler_bbr2_p16\",\n",
    "#         \"no-loss/pscheduler_bbr2_p16\"),\n",
    "\n",
    "#         (\"0.1-loss/pscheduler_cubic_p16\",\n",
    "#         \"0.1-loss.v1/pscheduler_cubic_p1\",   \n",
    "#         \"0.1-loss.v1/pscheduler_cubic_p16\",\n",
    "#         \"0.001-loss/pscheduler_cubic_p16\",\n",
    "#         \"0.01-loss.v1/pscheduler_cubic_p1\",  \n",
    "#         \"0.01-loss.v1/pscheduler_cubic_p16\",\n",
    "#         \"no-loss/pscheduler_cubic_p16\"),\n",
    "\n",
    "#         (\"0.001-loss/pscheduler_both_p16\",       \n",
    "#         \"0.1-loss.v1/pscheduler_both_p1\",       \n",
    "#         \"0.1-loss.v1/pscheduler_both_p16\",      \n",
    "#         \"0.01-loss.v1/pscheduler_both_p1\",      \n",
    "#         \"0.01-loss.v1/pscheduler_both_p16\",     \n",
    "#         \"0.1-loss.v2/pscheduler_both_p16\",      \n",
    "#         \"0.1-loss.v2/pscheduler_both_p16_bbr1\", \n",
    "#         \"no-loss/pscheduler_both_p16\")\n",
    "#         ]\n",
    "\n",
    "\n",
    "# # for p in paths:\n",
    "# for q1 in paths[0]:\n",
    "#     print(f\"============================ {q1} ============================\")\n",
    "#     pscheduler_bbr2  = os.path.join(rootdir, q1)\n",
    "#     filenames = traverse(pscheduler_bbr2)\n",
    "\n",
    "#     data_seg = []\n",
    "#     tput_bbr2_p1 = []\n",
    "#     key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "#     for i,f in enumerate(filenames):\n",
    "#         try:\n",
    "#             path = Path(f)\n",
    "#             data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "#             bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "#             for j,d in enumerate(data):\n",
    "#                 if \"interval\" in d.keys() and j==0:\n",
    "#                     start_time = data[j]['interval']['time'] # Start time of the test\n",
    "#                 elif key2 in data[j].keys():\n",
    "#                     end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "#                     mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "#                     bbr2_data_seg = data[j][key2]\n",
    "#                     data_seg.append( bbr2_data_seg )\n",
    "\n",
    "#                     throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "#                     tput_bbr2_p1.append( throughput )\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "\n",
    "#     print(\"Throughput (P1)\")\n",
    "#     print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p1):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p1):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p1)/np.mean(tput_bbr2_p1)):.5f}  |  Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "#     print(\"Data Segment (P1)\")\n",
    "#     print(f\"BBRv2 - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "#     print(\"\")\n",
    "\n",
    "# for q2 in paths[1]:\n",
    "#     print(f\"============================ {q2} ============================\")\n",
    "#     pscheduler_cubic = os.path.join(rootdir, q2)\n",
    "#     filenames = traverse(pscheduler_cubic)\n",
    "\n",
    "#     data_seg = []\n",
    "#     tput_p1_cubic = []\n",
    "#     key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "#     for i,f in enumerate(filenames):\n",
    "#         try:\n",
    "#             path = Path(f)\n",
    "#             # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "#             data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "#             cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "#             for j,d in enumerate(data):\n",
    "#                 if \"interval\" in d.keys() and j==0:\n",
    "#                     start_time = data[j]['interval']['time'] # Start time of the test\n",
    "#                 elif key1 in data[j].keys():\n",
    "#                     end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "#                     mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "#                     cubic_data_seg = data[j][key1]\n",
    "#                     data_seg.append( cubic_data_seg )\n",
    "\n",
    "#                     throughput = (cubic_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "#                     tput_p1_cubic.append( throughput )\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "\n",
    "#     print(\"Throughput (P1)\")\n",
    "#     print(f\"CUBIC - Mean: {np.mean(tput_p1_cubic):.5f}  |  Std. Dev.: {np.std(tput_p1_cubic):.5f}  |  Coef. of Variance: {(np.std(tput_p1_cubic)/np.mean(tput_p1_cubic)):.5f}  |  Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "#     print(\"Data Segment (P1)\")\n",
    "#     print(f\"CUBIC - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n",
    "#     print(\"\")\n",
    "\n",
    "# for q3 in paths[2]:\n",
    "#     print(f\"============================ {q3} ============================\")\n",
    "#     pscheduler_both = os.path.join(rootdir, q3)\n",
    "#     filenames = traverse(pscheduler_both)\n",
    "\n",
    "#     data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "#     tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "#     key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "#     for i,f in enumerate(filenames):\n",
    "#         try:\n",
    "#             path = Path(f)\n",
    "#             data = [json.loads(line) for line in open(f, 'r')]\n",
    "            \n",
    "#             throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "#             for j,d in zip(range(len(data)),data):\n",
    "#                 try:\n",
    "#                     if \"interval\" in d.keys() and j==0:\n",
    "#                         start_time = d['interval']['time'] # Start time of the test\n",
    "#                     if \"streams\" in d.keys():\n",
    "#                         end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "#                         mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                        \n",
    "#                         cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "#                         for j in range(len(d['streams'])):\n",
    "#                             if \"cubic\" in d['streams'][j]['cc']:\n",
    "#                                 cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "#                             elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "#                                 bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        \n",
    "#                         data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "#                         throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "#                         tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "#                         data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "#                         throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "#                         tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(e)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "\n",
    "#     print(\"Throughput (P16)\")\n",
    "#     print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}  |  Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "#     print(f\"CUBIC - Mean: {np.mean(tput_cubic_p16):.5f}  |  Std. Dev.: {np.std(tput_cubic_p16):.5f}  |  Coef. of Variance: {(np.std(tput_cubic_p16)/np.mean(tput_cubic_p16)):.5f}  |  Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "\n",
    "#     print(\"Data Segment (P16)\")\n",
    "#     print(f\"BBRv2 - Mean: {np.mean(data_seg_sum_bbr2):.5f}  |  Std. Dev.: {np.std(data_seg_sum_bbr2):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_bbr2)/np.mean(data_seg_sum_bbr2)):.5f}  |  Variance: {np.var(data_seg_sum_bbr2):.5f}\")\n",
    "#     print(f\"CUBIC - Mean: {np.mean(data_seg_sum_cubic):.5f}  |  Std. Dev.: {np.std(data_seg_sum_cubic):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_cubic)/np.mean(data_seg_sum_cubic)):.5f}  |  Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "#     print(50*\"=\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================ 0.1-loss.v1/pscheduler_bbr2_p1 ============================\n",
      "Total files: 35\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 2.22921  |  Std. Dev.: 0.03997  |  Coef. of Variance: 0.01793  |  Variance: 0.00160\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 9351449.48571  |  Std. Dev.: 173773.47248  |  Coef. of Variance: 0.01858  |  Variance: 30197219736.82122\n",
      "\n",
      "============================ 0.001-loss/pscheduler_bbr2_p16 ============================\n",
      "Total files: 70\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 9.79684  |  Std. Dev.: 0.05101  |  Coef. of Variance: 0.00521  |  Variance: 0.00260\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 41181799.81429  |  Std. Dev.: 229777.31653  |  Coef. of Variance: 0.00558  |  Variance: 52797615192.86552\n",
      "\n",
      "============================ 0.1-loss.v1/pscheduler_bbr2_p16 ============================\n",
      "Total files: 35\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 9.80138  |  Std. Dev.: 0.03258  |  Coef. of Variance: 0.00332  |  Variance: 0.00106\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 41214670.34286  |  Std. Dev.: 134867.55946  |  Coef. of Variance: 0.00327  |  Variance: 18189258594.79673\n",
      "\n",
      "============================ 0.01-loss.v1/pscheduler_bbr2_p1 ============================\n",
      "Total files: 20\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 2.33679  |  Std. Dev.: 0.01929  |  Coef. of Variance: 0.00826  |  Variance: 0.00037\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 9796732.40000  |  Std. Dev.: 75634.88146  |  Coef. of Variance: 0.00772  |  Variance: 5720635293.74000\n",
      "\n",
      "============================ 0.01-loss.v1/pscheduler_bbr2_p16 ============================\n",
      "Total files: 20\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 9.77223  |  Std. Dev.: 0.05096  |  Coef. of Variance: 0.00521  |  Variance: 0.00260\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 41019970.00000  |  Std. Dev.: 264980.43557  |  Coef. of Variance: 0.00646  |  Variance: 70214631235.20000\n",
      "\n",
      "============================ no-loss/pscheduler_bbr2_p16 ============================\n",
      "Total files: 70\n",
      "Throughput (P1)\n",
      "BBRv2 - Mean: 9.79786  |  Std. Dev.: 0.05066  |  Coef. of Variance: 0.00517  |  Variance: 0.00257\n",
      "Data Segment (P1)\n",
      "BBRv2 - Mean: 41188010.45714  |  Std. Dev.: 222407.69303  |  Coef. of Variance: 0.00540  |  Variance: 49465181920.87674\n",
      "\n",
      "============================ 0.1-loss/pscheduler_cubic_p16 ============================\n",
      "Total files: 19\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 3.56209  |  Std. Dev.: 2.97471  |  Coef. of Variance: 0.83510  |  Variance: 8.84891\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 14948659.42105  |  Std. Dev.: 12472814.72969  |  Coef. of Variance: 0.83438  |  Variance: 155571107281283.18750\n",
      "\n",
      "============================ 0.1-loss.v1/pscheduler_cubic_p1 ============================\n",
      "Total files: 35\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 0.17576  |  Std. Dev.: 0.17211  |  Coef. of Variance: 0.97925  |  Variance: 0.02962\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 736754.91429  |  Std. Dev.: 721018.83127  |  Coef. of Variance: 0.97864  |  Variance: 519868155051.56409\n",
      "\n",
      "============================ 0.1-loss.v1/pscheduler_cubic_p16 ============================\n",
      "Total files: 35\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 2.76638  |  Std. Dev.: 2.68914  |  Coef. of Variance: 0.97208  |  Variance: 7.23149\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 11606901.91429  |  Std. Dev.: 11265964.00774  |  Coef. of Variance: 0.97063  |  Variance: 126921945023711.06250\n",
      "\n",
      "============================ 0.001-loss/pscheduler_cubic_p16 ============================\n",
      "Total files: 70\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 9.84790  |  Std. Dev.: 0.04865  |  Coef. of Variance: 0.00494  |  Variance: 0.00237\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 41387698.40299  |  Std. Dev.: 117717.07736  |  Coef. of Variance: 0.00284  |  Variance: 13857310301.19581\n",
      "\n",
      "============================ 0.01-loss.v1/pscheduler_cubic_p1 ============================\n",
      "Total files: 20\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 0.85261  |  Std. Dev.: 0.61914  |  Coef. of Variance: 0.72618  |  Variance: 0.38334\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 3573283.80000  |  Std. Dev.: 2593679.49017  |  Coef. of Variance: 0.72585  |  Variance: 6727173297712.16113\n",
      "\n",
      "============================ 0.01-loss.v1/pscheduler_cubic_p16 ============================\n",
      "Total files: 20\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 7.76815  |  Std. Dev.: 2.43479  |  Coef. of Variance: 0.31343  |  Variance: 5.92818\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 32589560.70000  |  Std. Dev.: 10182836.66929  |  Coef. of Variance: 0.31246  |  Variance: 103690162633341.81250\n",
      "\n",
      "============================ no-loss/pscheduler_cubic_p16 ============================\n",
      "Total files: 70\n",
      "Throughput (P1)\n",
      "CUBIC - Mean: 9.85533  |  Std. Dev.: 0.03247  |  Coef. of Variance: 0.00329  |  Variance: 0.00105\n",
      "Data Segment (P1)\n",
      "CUBIC - Mean: 41422680.24286  |  Std. Dev.: 30625.86649  |  Coef. of Variance: 0.00074  |  Variance: 937943698.52673\n",
      "\n",
      "============================ 0.001-loss/pscheduler_both_p16 ============================\n",
      "Total files: 70\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 4.63356  |  Std. Dev.: 0.53610  |  Coef. of Variance: 0.11570  |  Variance: 0.28741\n",
      "CUBIC - Mean: 5.21819  |  Std. Dev.: 0.56052  |  Coef. of Variance: 0.10742  |  Variance: 0.31418\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 19480184.20000  |  Std. Dev.: 2284394.58253  |  Coef. of Variance: 0.11727  |  Variance: 5218458608701.56055\n",
      "CUBIC - Mean: 21929363.35714  |  Std. Dev.: 2313512.57031  |  Coef. of Variance: 0.10550  |  Variance: 5352340412994.77344\n",
      "==================================================\n",
      "============================ 0.1-loss.v1/pscheduler_both_p1 ============================\n",
      "Total files: 35\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 0.00000  |  Std. Dev.: 0.00000  |  Coef. of Variance: nan  |  Variance: 0.00000\n",
      "CUBIC - Mean: 0.17595  |  Std. Dev.: 0.17241  |  Coef. of Variance: 0.97984  |  Variance: 0.02972\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 0.00000  |  Std. Dev.: 0.00000  |  Coef. of Variance: nan  |  Variance: 0.00000\n",
      "CUBIC - Mean: 737578.97143  |  Std. Dev.: 722300.50544  |  Coef. of Variance: 0.97929  |  Variance: 521718020152.71362\n",
      "==================================================\n",
      "============================ 0.1-loss.v1/pscheduler_both_p16 ============================\n",
      "Total files: 35\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-7b9ef5707892>:149: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}  |  Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
      "<ipython-input-4-7b9ef5707892>:153: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  print(f\"BBRv2 - Mean: {np.mean(data_seg_sum_bbr2):.5f}  |  Std. Dev.: {np.std(data_seg_sum_bbr2):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_bbr2)/np.mean(data_seg_sum_bbr2)):.5f}  |  Variance: {np.var(data_seg_sum_bbr2):.5f}\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Throughput (P16)\n",
      "BBRv2 - Mean: 8.57664  |  Std. Dev.: 1.13554  |  Coef. of Variance: 0.13240  |  Variance: 1.28945\n",
      "CUBIC - Mean: 1.22878  |  Std. Dev.: 1.16579  |  Coef. of Variance: 0.94874  |  Variance: 1.35908\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 36063661.57143  |  Std. Dev.: 4835482.92428  |  Coef. of Variance: 0.13408  |  Variance: 23381895111017.26953\n",
      "CUBIC - Mean: 5156028.65714  |  Std. Dev.: 4883125.86587  |  Coef. of Variance: 0.94707  |  Variance: 23844918221903.59375\n",
      "==================================================\n",
      "============================ 0.01-loss.v1/pscheduler_both_p1 ============================\n",
      "Total files: 20\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 0.00000  |  Std. Dev.: 0.00000  |  Coef. of Variance: nan  |  Variance: 0.00000\n",
      "CUBIC - Mean: 0.86197  |  Std. Dev.: 0.61398  |  Coef. of Variance: 0.71229  |  Variance: 0.37697\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 0.00000  |  Std. Dev.: 0.00000  |  Coef. of Variance: nan  |  Variance: 0.00000\n",
      "CUBIC - Mean: 3612716.70000  |  Std. Dev.: 2572222.77993  |  Coef. of Variance: 0.71199  |  Variance: 6616330029610.70996\n",
      "==================================================\n",
      "============================ 0.01-loss.v1/pscheduler_both_p16 ============================\n",
      "Total files: 20\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 6.62652  |  Std. Dev.: 1.31460  |  Coef. of Variance: 0.19838  |  Variance: 1.72818\n",
      "CUBIC - Mean: 3.20701  |  Std. Dev.: 1.34362  |  Coef. of Variance: 0.41896  |  Variance: 1.80532\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 27818296.15000  |  Std. Dev.: 5549826.07535  |  Coef. of Variance: 0.19950  |  Variance: 30800569466633.22656\n",
      "CUBIC - Mean: 13453130.85000  |  Std. Dev.: 5621999.43261  |  Coef. of Variance: 0.41790  |  Variance: 31606877620289.33594\n",
      "==================================================\n",
      "============================ 0.1-loss.v2/pscheduler_both_p16 ============================\n",
      "Total files: 4\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 8.13379  |  Std. Dev.: 1.37580  |  Coef. of Variance: 0.16915  |  Variance: 1.89283\n",
      "CUBIC - Mean: 1.67969  |  Std. Dev.: 1.40782  |  Coef. of Variance: 0.83814  |  Variance: 1.98197\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 34190273.25000  |  Std. Dev.: 5847747.62537  |  Coef. of Variance: 0.17104  |  Variance: 34196152290056.18750\n",
      "CUBIC - Mean: 7046142.50000  |  Std. Dev.: 5898837.31589  |  Coef. of Variance: 0.83717  |  Variance: 34796281679336.75000\n",
      "==================================================\n",
      "============================ 0.1-loss.v2/pscheduler_both_p16_bbr1 ============================\n",
      "Total files: 4\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 0.00000  |  Std. Dev.: 0.00000  |  Coef. of Variance: nan  |  Variance: 0.00000\n",
      "CUBIC - Mean: 1.51346  |  Std. Dev.: 1.14808  |  Coef. of Variance: 0.75858  |  Variance: 1.31809\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 0.00000  |  Std. Dev.: 0.00000  |  Coef. of Variance: nan  |  Variance: 0.00000\n",
      "CUBIC - Mean: 6348735.25000  |  Std. Dev.: 4807693.82026  |  Coef. of Variance: 0.75727  |  Variance: 23113919869326.68750\n",
      "==================================================\n",
      "============================ no-loss/pscheduler_both_p16 ============================\n",
      "Total files: 70\n",
      "Throughput (P16)\n",
      "BBRv2 - Mean: 3.25683  |  Std. Dev.: 0.42412  |  Coef. of Variance: 0.13023  |  Variance: 0.17988\n",
      "CUBIC - Mean: 6.59831  |  Std. Dev.: 0.43426  |  Coef. of Variance: 0.06581  |  Variance: 0.18858\n",
      "Data Segment (P16)\n",
      "BBRv2 - Mean: 13690160.87143  |  Std. Dev.: 1789237.17237  |  Coef. of Variance: 0.13070  |  Variance: 3201369658997.62598\n",
      "CUBIC - Mean: 27731902.62857  |  Std. Dev.: 1798209.62528  |  Coef. of Variance: 0.06484  |  Variance: 3233557856457.00537\n",
      "==================================================\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('bbrv2': conda)"
  },
  "interpreter": {
   "hash": "f6e2000e74d83ab38a7a73c8353776f8595191be383670aed524d2a15b88663d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}