{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cpu available.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "# --- System ---\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import errno\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Utility ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Pytorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# --- sklearn ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Found {device} available.\")\n",
    "rootdir = os.getcwd()\n",
    "\n",
    "try:\n",
    "    os.makedirs(os.path.join(rootdir,'checkpoint'))\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise\n",
    "\n",
    "\n",
    "class clr:\n",
    "    \"\"\"\n",
    "    Defining colors for the print syntax coloring\n",
    "    \"\"\"\n",
    "    H   = '\\033[35m' # Header\n",
    "    B   = '\\033[94m' # Blue\n",
    "    G   = '\\033[36m' # Green\n",
    "    W   = '\\033[93m' # Warning\n",
    "    F   = '\\033[91m' # Fail\n",
    "    E   = '\\033[0m'  # End\n",
    "    BD  = '\\033[1m'  # Bold\n",
    "    UL  = '\\033[4m'  # Underline\n",
    "\n",
    "\n",
    "class SEEDEVERYTHING:\n",
    "    # random weight initialization\n",
    "    def __init__(self):\n",
    "        self.seed = 42\n",
    "\n",
    "    def _weight_init_(self):\n",
    "        random.seed(self.seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        print(\"Seeded everything.\")\n",
    "\n",
    "\n",
    "class DATA:\n",
    "    \"\"\"\n",
    "    Providing all the preprocessing techniques such as;\n",
    "    - Data Loading,\n",
    "    - Data Cleaning,\n",
    "    - One-Hot Encoding,\n",
    "    - Data Normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, infile):\n",
    "        self.infile = infile\n",
    "\n",
    "    def _df_load_and_clean(self, infile):\n",
    "        # data loading and preprocessing\n",
    "        dataPath = self.infile\n",
    "        df = pd.read_csv(dataPath)\n",
    "\n",
    "        # Dropping columns that are not required at the moment\n",
    "        df = df.drop(columns=['Unnamed: 0','UUID','HOSTNAME','TIMESTAMP',\n",
    "                              'THROUGHPUT (Receiver)','LATENCY (mean)',\n",
    "                              'CONGESTION (Receiver)','BYTES (Receiver)'])\n",
    "        return df\n",
    "\n",
    "    def _preprocessing(self, df):\n",
    "        print(\"\\nStarted preprocessing ...\")\n",
    "\n",
    "        # Spliting 1gbps -> 1, gbps\n",
    "        pacing = df['PACING'].values\n",
    "        for i, p in enumerate(pacing):\n",
    "            v, _ = p.split(\"gbit\")\n",
    "            pacing[i] = float(v)\n",
    "        df['PACING'] = pacing\n",
    "\n",
    "        # Dropping rows with pacing rate 10.5, glitch in the training data\n",
    "        df.drop( df[ df['PACING'] == 10.5 ].index, inplace=True)\n",
    "\n",
    "        # Supervised training approach needs total number of classes for classification task\n",
    "        num_of_classes = len(df['PACING'].unique())\n",
    "\n",
    "        print(f\"Using the following features:\\n{clr.G}{df.columns.values}{clr.E}\\n\")\n",
    "\n",
    "        \"\"\"\n",
    "        Transform between iterable of iterables and a multilabel format.\n",
    "        Although a list of sets or tuples is a very intuitive format for\n",
    "        multilabel data, it is unwieldy to process. This transformer converts\n",
    "        between this intuitive format and the supported multilabel format\n",
    "        \"\"\"\n",
    "        mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "        alias_df = df.join(pd.DataFrame.sparse.from_spmatrix(mlb.fit_transform(df.pop('ALIAS')),\n",
    "                                                            index=df.index,\n",
    "                                                            columns=mlb.classes_))\n",
    "\n",
    "        df_ = alias_df.join(pd.DataFrame.sparse.from_spmatrix(mlb.fit_transform(alias_df.pop('CONGESTION (Sender)')),\n",
    "                                                             index=alias_df.index,\n",
    "                                                             columns=mlb.classes_),\n",
    "                            how = 'left', lsuffix='left', rsuffix='right')\n",
    "\n",
    "        X = df_[df_.columns.values].values\n",
    "        y = df_['PACING'].values\n",
    "        y = y.astype('float')\n",
    "\n",
    "        \"\"\"\n",
    "        Normalization: This estimator scales and translates each feature individually \n",
    "        such that it is in the given range on the training set\n",
    "        \"\"\"\n",
    "        minmax_scale = preprocessing.MinMaxScaler().fit(df_[df_.columns.values])\n",
    "        df_minmax = minmax_scale.transform(df_[df_.columns.values])\n",
    "\n",
    "        final_df = pd.DataFrame(df_minmax, columns=df_.columns.values)\n",
    "        X = final_df[df_.columns.values].values\n",
    "\n",
    "        return X, y, num_of_classes\n",
    "\n",
    "\n",
    "class RECEIVEFEATURES:\n",
    "    def __init__(self):\n",
    "        self.inputFea = []\n",
    "        self.statfile = '~/.json'\n",
    "\n",
    "    def _file_reader(self):\n",
    "        # Get data through the json file\n",
    "        jsonfile = open(self.statfile, \"r\")\n",
    "        data = json.loads(jsonfile)\n",
    "\n",
    "    def _read_buffer(self):\n",
    "        # Get the statistics data from the buffer of pre-src-cmd\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom data loader for ELK stack dataset\n",
    "class PACINGDATASET(Dataset):\n",
    "    \"\"\"\n",
    "    TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        y = self.tensors[1][index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "\n",
    "# model definition\n",
    "class PACINGCLASSIFIER (nn.Module):\n",
    "    \"\"\"\n",
    "    Pacing Classifier is a supervised approach to the pacing\n",
    "    prediction task (assuming interface limit 10G)\n",
    "    \"\"\"\n",
    "    def __init__(self, nc=21, inputFeatures=7):\n",
    "        super(PACINGCLASSIFIER, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(inputFeatures, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 128)\n",
    "        self.fc4 = torch.nn.Linear(128, 128)\n",
    "        self.fc5 = torch.nn.Linear(128, 64)\n",
    "        self.fc6 = torch.nn.Linear(64, nc)\n",
    "\n",
    "        \"\"\"\n",
    "        Fills the input Tensor with values according to the method\n",
    "        described in \"Understanding the difficulty of training deep\n",
    "        feedforward neural networks\" - Glorot, X. & Bengio, Y. (2010),\n",
    "        using a uniform distribution. The resulting tensor will have\n",
    "        values sampled from mathcal{U}(-a, a)U(âˆ’a,a)\n",
    "        \"\"\"\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.zeros_(self.fc2.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        torch.nn.init.zeros_(self.fc3.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        torch.nn.init.zeros_(self.fc4.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc5.weight)\n",
    "        torch.nn.init.zeros_(self.fc5.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc6.weight)\n",
    "        torch.nn.init.zeros_(self.fc6.bias)\n",
    "\n",
    "        self.lrelu = torch.nn.LeakyReLU(negative_slope=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.lrelu(self.fc1(x))\n",
    "        z = self.lrelu(self.fc2(z))\n",
    "        z = self.lrelu(self.fc3(z))\n",
    "        z = self.lrelu(self.fc4(z))\n",
    "        z = self.lrelu(self.fc5(z))\n",
    "        z = self.fc6(z)  # no activation\n",
    "        return z\n",
    "\n",
    "    def _train(self, args, model, trainloader, testloader, optimizer, scheduler, lossFunction):\n",
    "        # Model training on the retrieved statistics.\n",
    "        print(\"\")\n",
    "        print(\"Epoch\", \" Loss\", \"  Acc\", sep=' '*11, end=\"\\n\")\n",
    "        EPOCH = args.epoch\n",
    "        trainloss = []\n",
    "        \n",
    "        for epoch in range(0, EPOCH):\n",
    "            model.train()\n",
    "            torch.manual_seed(epoch+1)                      # recovery reproducibility\n",
    "            epoch_loss = 0                                  # for one full epoch\n",
    "\n",
    "            for (batch_idx, batch) in enumerate(trainloader):\n",
    "                (xs, ys) = batch                            # (predictors, targets)\n",
    "                xs, ys = xs.float(), ys.float()\n",
    "                optimizer.zero_grad()                       # prepare gradients\n",
    "\n",
    "                output = model(xs)                          # predicted pacing rate\n",
    "                loss = lossFunction(output, ys.long())      # avg per item in batch\n",
    "\n",
    "                epoch_loss += loss.item()                   # accumulate averages\n",
    "                loss.backward()                             # compute gradients\n",
    "                optimizer.step()                            # update weights\n",
    "            \n",
    "            scheduler.step()\n",
    "            trainloss.append(epoch_loss)\n",
    "            if epoch % args.interval == 0:\n",
    "\n",
    "                model.eval()                                # evaluation phase on every epoch\n",
    "                correct, acc = 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for xs, ys in testloader:\n",
    "                        xs, ys = xs.float(), ys.long()\n",
    "                        pred = torch.max(model(xs), 1)[1]\n",
    "                        correct += (pred == ys).sum().item()\n",
    "                    acc = (100 * float(correct / len(testloader.dataset)) )\n",
    "\n",
    "                print(f\"{epoch+0:03}/{EPOCH}\", f\"{epoch_loss:.4f}\", f\"{acc:.4f}\", sep=' '*10, end=\"\\n\")\n",
    "\n",
    "                dt = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "                fn = \"checkpoint/\" + str(dt) + str(\"-\") + str(epoch) + \"_ckpt.pt\"\n",
    "\n",
    "                info_dict = {\n",
    "                    'epoch' : epoch,\n",
    "                    'model_state' : model.state_dict(),\n",
    "                    'optimizer_state' : optimizer.state_dict()\n",
    "                }\n",
    "                if args.save:\n",
    "                    torch.save(info_dict, fn)               # save checkpoint\n",
    "\n",
    "        torch.save (info_dict, \"checkpoint/best.pt\")\n",
    "        print(\"\\n*************************\\nTraining complete\\n\")\n",
    "        return model\n",
    "\n",
    "    def _loadModel(self, fn, num_of_classes, inputFea):\n",
    "        # Load a pre-trained model from a given path\n",
    "        model = PACINGCLASSIFIER (nc=num_of_classes, inputFeatures=inputFea)\n",
    "        modelPath = torch.load(fn)\n",
    "        model.load_state_dict(modelPath['model_state'])\n",
    "        model.to(device)\n",
    "        print(\"\\n*************************\\nPre-trained model loaded\\n*************************\\n\")\n",
    "        return model\n",
    "\n",
    "    def _test(self, model, inputSample, inputFea):\n",
    "\n",
    "        if len(inputSample)==inputFea and isinstance(inputSample, list):\n",
    "            # converting the sample to tensor array\n",
    "            ukn = np.array([inputSample], dtype=np.float32)\n",
    "            sample = torch.tensor(ukn, dtype=torch.float32).to(device)\n",
    "\n",
    "        elif len(inputSample)!=inputFea and isinstance(inputSample, list):\n",
    "            # Needs pre-processing similar to model training\n",
    "\n",
    "            # converting the sample to tensor array\n",
    "            ukn = np.array([inputSample], dtype=np.float32)\n",
    "            sample = torch.tensor(ukn, dtype=torch.float32).to(device)\n",
    "        \n",
    "        elif isinstance(inputSample, torch.Tensor):\n",
    "            # Using a test data sample for Demo\n",
    "            sample = inputSample.float().unsqueeze_(0)\n",
    "\n",
    "\n",
    "        # Inference stage\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = torch.max(model(sample), 1)[1]\n",
    "        return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPacingRate(bufferData, phase='test'):\n",
    "\n",
    "    seeder = SEEDEVERYTHING()\n",
    "    seeder._weight_init_()\n",
    "\n",
    "    # Preprocessing\n",
    "    prep = DATA(\"data/statistics-5.csv\")\n",
    "    df = prep._df_load_and_clean(\"data/statistics-5.csv\")\n",
    "\n",
    "    if bufferData:\n",
    "        df = df.append({\n",
    "                        'ALIAS':\"hostA\", # bufferData[0],\n",
    "                        'STREAMS':bufferData[1],\n",
    "                        'PACING':\"6gbit\",\n",
    "                        'THROUGHPUT (Sender)':bufferData[2],\n",
    "                        'LATENCY (min.)':bufferData[3],\n",
    "                        'LATENCY (max.)':bufferData[4],\n",
    "                        'RETRANSMITS':bufferData[5],\n",
    "                        'CONGESTION (Sender)':bufferData[6],\n",
    "                        }, ignore_index=True)\n",
    "\n",
    "    X, y, num_of_classes = prep._preprocessing(df)\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "\n",
    "    # Dataset w/o any tranformations\n",
    "    data = PACINGDATASET(tensors=(X, y), transform=None)\n",
    "    dataloader  = torch.utils.data.DataLoader(data, batch_size=256)\n",
    "\n",
    "    inputFea = len(data[0][0])\n",
    "    print(\"Length of Input Feautures: \", inputFea)\n",
    "    print(\"Length of Input: \", len(bufferData))\n",
    "\n",
    "    model = PACINGCLASSIFIER (nc=num_of_classes, inputFeatures=inputFea)\n",
    "    print(\"\\n\", model)\n",
    "\n",
    "    fn = os.path.join(os.getcwd(), \"checkpoint/best.pt\")\n",
    "    print(f\"Current working directory: {fn}\")\n",
    "\n",
    "    if phase==\"test\" and os.path.exists(fn):\n",
    "        try:\n",
    "            # Get the features from iperf3 prob test\n",
    "\n",
    "            print(\"\\nInside the inference stage\")\n",
    "            # Load the model\n",
    "            inferenceModel = model._loadModel(fn, num_of_classes, inputFea)\n",
    "            print(f\"printing the input sample: {data[len(data)-1]}\\n\")\n",
    "            inputSample, groundtruth = data[len(data)-1]\n",
    "\n",
    "            pacing = model._test(inferenceModel, inputSample, inputFea)\n",
    "            print(f\"Predicted pacing rate: {clr.G}{pacing}{clr.E}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Exception error: {e}\")\n",
    "            print(f\"{clr.F}Note: checkpoint folder should contain a pre-trained model, or switch the training phase.{clr.E}\")\n",
    "\n",
    "    return pacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seeded everything.\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "BESTLOSS = 10\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Testpoint Statistics')\n",
    "\n",
    "# parser.add_argument('-p', '--phase', default=\"test\", type=str,\n",
    "#                     help='Training and Testing Phase. {train/test}')\n",
    "# parser.add_argument('--infile', default=\"data/statistics-5.csv\", type=str,\n",
    "#                     help='CSV file used for training the model.')\n",
    "\n",
    "# parser.add_argument('-e', '--epoch', default=300, type=int,\n",
    "#                     help='Total number of training epochs')\n",
    "# parser.add_argument('-b', '--batch', default=256, type=int,\n",
    "#                     help='Batch-size in dataloader')\n",
    "# parser.add_argument('-lr', '--learning-rate', default=0.001, type=int,\n",
    "#                     help='Learning rate for the optimizer')\n",
    "# parser.add_argument('-i', '--interval', default=25, type=int,\n",
    "#                     help='Print statement interval')\n",
    "# parser.add_argument('-s', '--save', action='store_true',\n",
    "#                     help='To save the checkpoints of the training model')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# print(\"\")\n",
    "# for arg in vars(args):\n",
    "#     print (\"%-15s: %s\"%(arg,getattr(args, arg)))\n",
    "\n",
    "phase = \"test\"\n",
    "infile = \"../data/statistics-5.csv\"\n",
    "epoch = 300\n",
    "batch = 256\n",
    "lr = 0.001\n",
    "interval = 25\n",
    "save = True\n",
    "\n",
    "print(\"\")\n",
    "seeder = SEEDEVERYTHING()\n",
    "seeder._weight_init_()\n",
    "\n",
    "# Preprocessing\n",
    "prep = DATA(infile)\n",
    "df = prep._df_load_and_clean(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALIAS</th>\n",
       "      <th>STREAMS</th>\n",
       "      <th>PACING</th>\n",
       "      <th>THROUGHPUT (Sender)</th>\n",
       "      <th>LATENCY (min.)</th>\n",
       "      <th>LATENCY (max.)</th>\n",
       "      <th>RETRANSMITS</th>\n",
       "      <th>CONGESTION (Sender)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>1gbit</td>\n",
       "      <td>1.623277e+09</td>\n",
       "      <td>30062.0</td>\n",
       "      <td>30264.5</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>1gbit</td>\n",
       "      <td>1.652145e+09</td>\n",
       "      <td>60206.5</td>\n",
       "      <td>60572.0</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>1gbit</td>\n",
       "      <td>9.833584e+08</td>\n",
       "      <td>91576.5</td>\n",
       "      <td>92073.0</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>2gbit</td>\n",
       "      <td>1.965511e+09</td>\n",
       "      <td>122954.0</td>\n",
       "      <td>123533.5</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>3gbit</td>\n",
       "      <td>2.946649e+09</td>\n",
       "      <td>154383.5</td>\n",
       "      <td>155109.0</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5121</td>\n",
       "      <td>hostD</td>\n",
       "      <td>1</td>\n",
       "      <td>2gbit</td>\n",
       "      <td>1.612733e+09</td>\n",
       "      <td>3131908.0</td>\n",
       "      <td>4538737.5</td>\n",
       "      <td>10626.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5122</td>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>2gbit</td>\n",
       "      <td>1.866827e+09</td>\n",
       "      <td>3163305.0</td>\n",
       "      <td>4570179.0</td>\n",
       "      <td>10626.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5123</td>\n",
       "      <td>hostC</td>\n",
       "      <td>1</td>\n",
       "      <td>1gbit</td>\n",
       "      <td>8.338280e+08</td>\n",
       "      <td>3243547.5</td>\n",
       "      <td>4651874.0</td>\n",
       "      <td>10626.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5124</td>\n",
       "      <td>hostB</td>\n",
       "      <td>1</td>\n",
       "      <td>1gbit</td>\n",
       "      <td>4.970197e+08</td>\n",
       "      <td>3298707.5</td>\n",
       "      <td>4709556.5</td>\n",
       "      <td>10635.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5125</td>\n",
       "      <td>hostA</td>\n",
       "      <td>1</td>\n",
       "      <td>1gbit</td>\n",
       "      <td>8.381282e+08</td>\n",
       "      <td>3378968.5</td>\n",
       "      <td>4791379.5</td>\n",
       "      <td>10636.0</td>\n",
       "      <td>cubic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5126 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ALIAS  STREAMS PACING  THROUGHPUT (Sender)  LATENCY (min.)  \\\n",
       "0     hostA        1  1gbit         1.623277e+09         30062.0   \n",
       "1     hostA        1  1gbit         1.652145e+09         60206.5   \n",
       "2     hostA        1  1gbit         9.833584e+08         91576.5   \n",
       "3     hostA        1  2gbit         1.965511e+09        122954.0   \n",
       "4     hostA        1  3gbit         2.946649e+09        154383.5   \n",
       "...     ...      ...    ...                  ...             ...   \n",
       "5121  hostD        1  2gbit         1.612733e+09       3131908.0   \n",
       "5122  hostA        1  2gbit         1.866827e+09       3163305.0   \n",
       "5123  hostC        1  1gbit         8.338280e+08       3243547.5   \n",
       "5124  hostB        1  1gbit         4.970197e+08       3298707.5   \n",
       "5125  hostA        1  1gbit         8.381282e+08       3378968.5   \n",
       "\n",
       "      LATENCY (max.)  RETRANSMITS CONGESTION (Sender)  \n",
       "0            30264.5       1535.0               cubic  \n",
       "1            60572.0       2879.0               cubic  \n",
       "2            92073.0       2879.0               cubic  \n",
       "3           123533.5       2879.0               cubic  \n",
       "4           155109.0       2879.0               cubic  \n",
       "...              ...          ...                 ...  \n",
       "5121       4538737.5      10626.0               cubic  \n",
       "5122       4570179.0      10626.0               cubic  \n",
       "5123       4651874.0      10626.0               cubic  \n",
       "5124       4709556.5      10635.0               cubic  \n",
       "5125       4791379.5      10636.0               cubic  \n",
       "\n",
       "[5126 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started preprocessing ...\n",
      "Using the following features:\n",
      "\u001b[36m['ALIAS' 'STREAMS' 'PACING' 'THROUGHPUT (Sender)' 'LATENCY (min.)'\n",
      " 'LATENCY (max.)' 'RETRANSMITS' 'CONGESTION (Sender)']\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret 'Sparse[int64, 0]' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b64b34716140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m X_train, X_test, y_train, y_test = train_test_split(X, y, \n\u001b[1;32m      4\u001b[0m                                             \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                             random_state=1)\n",
      "\u001b[0;32m<ipython-input-8-798f423d498d>\u001b[0m in \u001b[0;36m_preprocessing\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0msuch\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mrange\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mminmax_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mdf_minmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminmax_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    349\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[1;32m    350\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;31m# some data must have been converted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         msg = (\"Data with input dtype %s were all converted to %s%s.\"\n\u001b[0;32m--> 606\u001b[0;31m                % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype,\n\u001b[0m\u001b[1;32m    607\u001b[0m                   context))\n\u001b[1;32m    608\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret 'Sparse[int64, 0]' as a data type"
     ]
    }
   ],
   "source": [
    "X, y, num_of_classes = prep._preprocessing(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size=0.25,\n",
    "                                            random_state=1)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test  = torch.tensor(X_test)\n",
    "y_test  = torch.tensor(y_test)\n",
    "\n",
    "lossFunction  = nn.CrossEntropyLoss()\n",
    "# BCE = nn.BCELoss(reduction='mean')\n",
    "# MSE = nn.MSELoss(reduction='mean') # 'mean', 'sum'. 'none'\n",
    "\n",
    "# Dataset w/o any tranformations\n",
    "traindata   = PACINGDATASET(tensors=(X_train, y_train),\n",
    "                            transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(traindata,\n",
    "                                          shuffle=True,\n",
    "                                          batch_size=batch)\n",
    "\n",
    "testdata    = PACINGDATASET(tensors=(X_test, y_test),\n",
    "                            transform=None)\n",
    "testloader  = torch.utils.data.DataLoader(testdata,\n",
    "                                          shuffle=True,\n",
    "                                          batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFea = len(traindata[0][0])\n",
    "model = PACINGCLASSIFIER (nc=num_of_classes, inputFeatures=inputFea)\n",
    "print(\"\\n\", model)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=lr,\n",
    "                      momentum=0.9,\n",
    "                      weight_decay=5e-4,\n",
    "                      nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                 milestones=[200, 350],\n",
    "                                                 gamma=0.1)\n",
    "\n",
    "fn = os.path.join(os.getcwd(),\"checkpoint/best.pt\")\n",
    "if args.phase==\"test\" and os.path.exists(fn):\n",
    "    try:\n",
    "        # Get the features from iperf3 prob test\n",
    "        # inputFea = []\n",
    "        inferenceModel = model._loadModel(fn, num_of_classes, inputFea)\n",
    "\n",
    "        # bufferReader = RECEIVEFEATURES()\n",
    "        # inputSample  = bufferReader._read_buffer()\n",
    "        inputSample, groundtruth = testdata[100]\n",
    "\n",
    "        pacing = model._test(inferenceModel, inputSample, inputFea)\n",
    "        print(f\"Normalized Input Sample :\\n{clr.G}{inputSample}{clr.E}\")\n",
    "        print(f\"Groundtruth pacing rate: {clr.G}{groundtruth.item()}{clr.E}\\nPredicted pacing rate: {clr.G}{pacing}{clr.E}\\n\")\n",
    "    except:\n",
    "        # DO THE TRAINING\n",
    "        ckpt = model._train(args, model, trainloader, testloader, optimizer, scheduler, lossFunction)          \n",
    "\n",
    "elif args.phase==\"train\":\n",
    "    # DO THE TRAINING\n",
    "    ckpt = model._train(args, model, trainloader, testloader, optimizer, scheduler, lossFunction)\n",
    "\n",
    "    inferenceModel = model._loadModel(fn, num_of_classes, inputFea)\n",
    "    bufferReader = RECEIVEFEATURES()\n",
    "    # inputSample  = bufferReader._read_buffer()\n",
    "    inputSample, groundtruth = testdata[101]\n",
    "    pacing = model._test(inferenceModel, inputSample, inputFea)\n",
    "    print(f\"Normalized Input Sample :\\n{clr.G}{inputSample}{clr.E}\")\n",
    "    print(f\"Groundtruth pacing rate: {clr.G}{groundtruth.item()}{clr.E}\\nPredicted pacing rate:  {clr.G}{pacing}{clr.E}\\n\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6e2000e74d83ab38a7a73c8353776f8595191be383670aed524d2a15b88663d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit",
   "language": "python",
   "name": "python36764bit83caddc3772f4cca992bbefe0473dc13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
